{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Categorical Time-Series Analysis Introduction CategoricalTimeSeries.jl is a Julia package regrouping methods of categorical time-series analysis. The term categorical commonly refers to two types of data: nominal and ordinal . Nominal , or labeled values represent discrete units that have no intrinsic order , like common types of pet: Cat Dog Bird Ordinal values on the other hand represent discrete and ordered units, like the size of a coffee cup: Small Medium Large When categorical data is layed out in function of time, one speaks of categorical time-series . Often, especially when dealing with ordinal time-series, it is enough to map the different values to a set of integers to carry the analysis. However, when it is not sufficient CategoricalTimeSeries.jl is here to help. Overview The package lets you carry four main kind of analysis: Spectral analysis , Data clustering , correlations analysis and motif recognition . Other functionnalities are also avalaible (see misc.). These methods are agnostic to the type of data used (ordinal or nominal) as they do not rely on a pre-established ordering. Here is a quick overview of these methods, for more details go to the specific sections. Spectral analysis The standard approach to study spectral properties in categorical time-series is to map the different values to a set of numbers. While the overall shape of the spectrum is usually unaffected by this operation, peaks representing cyclic behaviors in the time-series can completely disappear depending on the choice of mapping. To tackle this issue, one can use the spectral envelope method. It was developed by David S. Stoffer in order to identify optimal mappings. Data clustering A categorical time-series might have many different possible values, but these values are rarely independant. Some categories can be loosely equivalent to one-another. For example, among the values \"male\" , \"female\" , \"house\" and \"car\" , it is evident that \"male\" and \"female\" refer to similar concepts and could be grouped in one single category. Data clustering is the task of identifying such relationships in the time-series in order to reduce it to a more efficient representation. Here, an implementation of the information bottleneck concept is used to this end. Motif recognition Time-series sometimes present repeating motifs (or patterns) that are worthwhile identifying. In categorical time-series, the lack of proper distance measurement complicates this task, nonetheless several methods have been developed to this end. An implementation using the concept of random projection is used here. Correlation analysis The notion of autocorrelation function is formally not defined for a categorical time-series. Yet, it might be of interest to know how inter-dependent the values of the time-series are. Efforts have been made to generalize the concept of linear correlations to categorical time-series. This package implements several of these methods. Installation The source code is available on GitHub , otherwise, CategoricalTimeSeries.jl can be installed with using Pkg Pkg.add(\"CategoricalTimeSeries\") To use it, you need to import it: using CategoricalTimeSeries","title":"Home"},{"location":"#categorical-time-series-analysis","text":"","title":"Categorical Time-Series Analysis"},{"location":"#introduction","text":"CategoricalTimeSeries.jl is a Julia package regrouping methods of categorical time-series analysis. The term categorical commonly refers to two types of data: nominal and ordinal . Nominal , or labeled values represent discrete units that have no intrinsic order , like common types of pet: Cat Dog Bird Ordinal values on the other hand represent discrete and ordered units, like the size of a coffee cup: Small Medium Large When categorical data is layed out in function of time, one speaks of categorical time-series . Often, especially when dealing with ordinal time-series, it is enough to map the different values to a set of integers to carry the analysis. However, when it is not sufficient CategoricalTimeSeries.jl is here to help.","title":"Introduction"},{"location":"#overview","text":"The package lets you carry four main kind of analysis: Spectral analysis , Data clustering , correlations analysis and motif recognition . Other functionnalities are also avalaible (see misc.). These methods are agnostic to the type of data used (ordinal or nominal) as they do not rely on a pre-established ordering. Here is a quick overview of these methods, for more details go to the specific sections.","title":"Overview"},{"location":"#spectral-analysis","text":"The standard approach to study spectral properties in categorical time-series is to map the different values to a set of numbers. While the overall shape of the spectrum is usually unaffected by this operation, peaks representing cyclic behaviors in the time-series can completely disappear depending on the choice of mapping. To tackle this issue, one can use the spectral envelope method. It was developed by David S. Stoffer in order to identify optimal mappings.","title":"Spectral analysis"},{"location":"#data-clustering","text":"A categorical time-series might have many different possible values, but these values are rarely independant. Some categories can be loosely equivalent to one-another. For example, among the values \"male\" , \"female\" , \"house\" and \"car\" , it is evident that \"male\" and \"female\" refer to similar concepts and could be grouped in one single category. Data clustering is the task of identifying such relationships in the time-series in order to reduce it to a more efficient representation. Here, an implementation of the information bottleneck concept is used to this end.","title":"Data clustering"},{"location":"#motif-recognition","text":"Time-series sometimes present repeating motifs (or patterns) that are worthwhile identifying. In categorical time-series, the lack of proper distance measurement complicates this task, nonetheless several methods have been developed to this end. An implementation using the concept of random projection is used here.","title":"Motif recognition"},{"location":"#correlation-analysis","text":"The notion of autocorrelation function is formally not defined for a categorical time-series. Yet, it might be of interest to know how inter-dependent the values of the time-series are. Efforts have been made to generalize the concept of linear correlations to categorical time-series. This package implements several of these methods.","title":"Correlation analysis"},{"location":"#installation","text":"The source code is available on GitHub , otherwise, CategoricalTimeSeries.jl can be installed with using Pkg Pkg.add(\"CategoricalTimeSeries\") To use it, you need to import it: using CategoricalTimeSeries","title":"Installation"},{"location":"Correlations/","text":"Correlations The study of categorical data prevents the usage of standard tools like the autocorrelation function, as they are often not defined. The following functions provide ways to study categorical serial dependences. Most of these methods are described in C. Weiss's book \" An Introduction to Discrete-Valued Time Series \" (2018)[1]. Main functions cramer_coefficient \u2014 Function cramer_coefficient(series, lags) Measures average association between elements of series at time t and time t + lags . Cramer's V is an unsigned measurement : its values lies in [0,1], 0 being perfect independence and 1 perfect dependence. k can be biased, for more informations, refer to [1]. Parameters : series ( Array{Any,1} ): 1-D Array containing input categorical time-series. lags ( Array{Int,1} ): lag values at which cramer's coefficient is computed. Alternatively, lags can be an integer, a single integer value will then be returned. Returns : V , the value of cramer's coefficient for each value in lags . cohen_coefficient \u2014 Function cohen_coefficient(series, lags) Measures average association between elements of series at time t and time t + lags . Cohen's k is a signed measurement : its values lie in [-pe/(1 -pe), 1], with positive (negative) values indicating positive (negative) serial dependence at lags . pe is probability of agreement by chance. Parameters : series ( Array{Any,1} ): 1-D Array containing input categorical time-series. lags ( Array{Int,1} ): lag values at which Cohen's coefficient is computed. Alternatively, lags can be an integer, a single integer value will then be returned. Returns : K , the value of Cohen's coefficient for each value in lags . theils_u \u2014 Function theils_u(series, Lags) Measures average portion of information known about series at t + lags given that series is known at time t. Theil's U makes use of concepts borrowed from information theory U is an unsigned measurement: its values lies in [0,1], 0 meaning no information shared and 1 complete knowledge (determinism). Parameters : series ( Array{Any,1} ): 1-D Array containing input categorical time-series. lags ( Array{Int,1} ): lag values at which Theil's U is computed. Alternatively, lags can be an integer, a single integer value will then be returned. Returns : U , the value of Theil's U for each value in lags . Confidence interval Depending on the length of the time-series and the method used, the estimated value of serial dependence might fluctuate a lot around its true value. It is therefore useful to relate estimations to a corresponding confidence interval to know how significant given results are. The following function provides a confidence interval via bootstrap: bootstrap_CI \u2014 Function bootstrap_CI(series, lags, coef_func, n_iter = 1000) Returns a top and bottom limit of a 95% confidence interval at values of lags . The returned confidence interval corresponds to the null hypothesis (no serial dependence), if the estimated serial dependence lies in this interval, no significant correlations can be claimed. Parameters : series ( Array{Any,1} ): 1-D Array containing input categorical time-series. lags ( Array{Int,1} ): lag values at which the CI is computed. coef_func ( function ): the function for which the CI needs to be computed. coef_func can be one of the following functions : cramer_coefficient , cohen_coefficient or theils_U . n_iter ( Int ): number of iterations for the bootstrap procedure. The higher, the more precise but more computationaly demanding. Defaults to 1000. Returns : (top_values, bottom_values) , the top and bottom limit for the 95% CI, for each point in lags . Example Using the Pewee birdsong data (1943) one can do a serial dependence plot using Cohen's cofficient as follow : using DelimitedFiles, Plots using CategoricalTimeSeries #reading 'pewee' time-series test folder. data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"pewee.txt\") series = readdlm(data_path,',')[1,:] lags = collect(1:25) v = cohen_coefficient(series, lags) t, b = bootstrap_CI(series, cramer_coefficient, lags) a = plot(lags, v, xlabel = \"Lags\", ylabel = \"K\", label = \"Cramer's k\") plot!(a, lags, t, color = \"red\", label = \"Limits of 95% CI\"); plot!(a, lags, b, color = \"red\", label = \"\") [1] DOI : 10.1002/9781119097013","title":"Correlations"},{"location":"Correlations/#correlations","text":"The study of categorical data prevents the usage of standard tools like the autocorrelation function, as they are often not defined. The following functions provide ways to study categorical serial dependences. Most of these methods are described in C. Weiss's book \" An Introduction to Discrete-Valued Time Series \" (2018)[1].","title":"Correlations"},{"location":"Correlations/#main-functions","text":"cramer_coefficient \u2014 Function cramer_coefficient(series, lags) Measures average association between elements of series at time t and time t + lags . Cramer's V is an unsigned measurement : its values lies in [0,1], 0 being perfect independence and 1 perfect dependence. k can be biased, for more informations, refer to [1]. Parameters : series ( Array{Any,1} ): 1-D Array containing input categorical time-series. lags ( Array{Int,1} ): lag values at which cramer's coefficient is computed. Alternatively, lags can be an integer, a single integer value will then be returned. Returns : V , the value of cramer's coefficient for each value in lags . cohen_coefficient \u2014 Function cohen_coefficient(series, lags) Measures average association between elements of series at time t and time t + lags . Cohen's k is a signed measurement : its values lie in [-pe/(1 -pe), 1], with positive (negative) values indicating positive (negative) serial dependence at lags . pe is probability of agreement by chance. Parameters : series ( Array{Any,1} ): 1-D Array containing input categorical time-series. lags ( Array{Int,1} ): lag values at which Cohen's coefficient is computed. Alternatively, lags can be an integer, a single integer value will then be returned. Returns : K , the value of Cohen's coefficient for each value in lags . theils_u \u2014 Function theils_u(series, Lags) Measures average portion of information known about series at t + lags given that series is known at time t. Theil's U makes use of concepts borrowed from information theory U is an unsigned measurement: its values lies in [0,1], 0 meaning no information shared and 1 complete knowledge (determinism). Parameters : series ( Array{Any,1} ): 1-D Array containing input categorical time-series. lags ( Array{Int,1} ): lag values at which Theil's U is computed. Alternatively, lags can be an integer, a single integer value will then be returned. Returns : U , the value of Theil's U for each value in lags .","title":"Main functions"},{"location":"Correlations/#confidence-interval","text":"Depending on the length of the time-series and the method used, the estimated value of serial dependence might fluctuate a lot around its true value. It is therefore useful to relate estimations to a corresponding confidence interval to know how significant given results are. The following function provides a confidence interval via bootstrap: bootstrap_CI \u2014 Function bootstrap_CI(series, lags, coef_func, n_iter = 1000) Returns a top and bottom limit of a 95% confidence interval at values of lags . The returned confidence interval corresponds to the null hypothesis (no serial dependence), if the estimated serial dependence lies in this interval, no significant correlations can be claimed. Parameters : series ( Array{Any,1} ): 1-D Array containing input categorical time-series. lags ( Array{Int,1} ): lag values at which the CI is computed. coef_func ( function ): the function for which the CI needs to be computed. coef_func can be one of the following functions : cramer_coefficient , cohen_coefficient or theils_U . n_iter ( Int ): number of iterations for the bootstrap procedure. The higher, the more precise but more computationaly demanding. Defaults to 1000. Returns : (top_values, bottom_values) , the top and bottom limit for the 95% CI, for each point in lags .","title":"Confidence interval"},{"location":"Correlations/#example","text":"Using the Pewee birdsong data (1943) one can do a serial dependence plot using Cohen's cofficient as follow : using DelimitedFiles, Plots using CategoricalTimeSeries #reading 'pewee' time-series test folder. data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"pewee.txt\") series = readdlm(data_path,',')[1,:] lags = collect(1:25) v = cohen_coefficient(series, lags) t, b = bootstrap_CI(series, cramer_coefficient, lags) a = plot(lags, v, xlabel = \"Lags\", ylabel = \"K\", label = \"Cramer's k\") plot!(a, lags, t, color = \"red\", label = \"Limits of 95% CI\"); plot!(a, lags, b, color = \"red\", label = \"\") [1] DOI : 10.1002/9781119097013","title":"Example"},{"location":"Data_clustering/","text":"Information bottleneck The information bottleneck (IB) concept can be used in the context of categorical data analysis to do clustering , or in other words, to look for categories which have equivalent functions. Given a time-series, the IB looks for a concise representation of the data that preserves as much meaningful information as possible. In a sense, it is a lossy compression algorithm. The information to preserve can be seen as the ability to make predictions: given a specific context, how much of what is coming next can we predict ? The goal of this algorithm is to cluster categorical data while preserving predictive power. To learn more about the information bottleneck you can look at [ 1 ] or [ 2 ] Quick start To do a simple IB clustering of categorical, the first step is to instantiate an IB model. Then optimize it via the IB_optimize! function to obtain to obtain the optimal parameters. data = readdlm(\"/path/to/data/\") model = IB(data) #you can call IB(x, beta). beta is a real number that controls the amount of compression. IB_optimize!(model) The data needs to be presented as a 1-D array, otherwise IB interprets it as a probability distribution (see below). To see the results, you can use: print_results(model) Rows are clusters and columns correspond to the input categories. The result is the probability p(t|x) of a category belonging to a given cluster. Since most of the probabilities are very low, print_results sets every p(t|x) > 0.1 to 1 . p(t|x) < 0.1 are set to 0 otherwise for ease of readability (see further usage for more options). Further usage To have a better grasp of the results produced by IB clustering, it is important to understand the parameters influencing the algorithm of IB model structures. The two most important parameters are the amount compression and the definition of the context . They are provided upon instanciation: IB \u2014 Type IB(x, y, \u03b2 = 100, algorithm = \"IB\") IB(x, \u03b2 = 100, algorithm = \"IB\") IB(pxy::Array{Float64,2}, \u03b2 = 100, algorithm = \"IB\") Parameters : x ( Array{Int,1} or Array{Float,1} ): 1-D Array containing input categorical time-series. y ( Array{Any,1} ): Context used for data compression. If not provided, defaults to \"next element\", meaning for each element of x, y represent the next element in the series. This means that the IB model will try to preserve as much information between 'x' and it's next element. (see get_y function) \u03b2 ( Int ): parameter controlling the degree of compression. The smaller \u03b2 is, the more compression. The higher \u03b2 , the bigger the mutual information I(X;T) between the final clusters and original categories is. There are two undesirable situations: if \u03b2 is too small, maximal compression is achieved and all information is lost. If \u03b2 is too high, there is no compression. with \"IB\" algorithm, a high \u03b2 value (~200) is a good starting point. With \"DIB\" algorithm, \u03b2 > ~5 can already be too high to achieve any compression. \u03b2 values > ~1000 break optimization because all metrics are effectively 0. algorithm ( String ): The kind of compression algorithm to use. \"IB\" choses the original IB algorithm (Tishby, 1999) which does soft clustering, \"DIB\" choses the deterministic IB algorithm (DJ Strouse, 2016) doing hard clustering. The former seems to produce more meaningfull clustering. Defaults to \"IB\". pxy ( Array{Float,2} ): joint probability of element 'x' to occur with context 'y'. If not provided, is computed automatically. From x and y . Returns : instance of the IB mutable struct. get_y \u2014 Function get_y(data, type = \"nn\") Defines and return the context associated with the input time-series data . Parameters : data ( Array{Any,1} ): 1-D Array containing input categorical time-series. type ( String ): type of context to use. Possible values are \"nn\" or \"an\". Defaults to \"nn\" (for next neighbor ). This means, if data = [\"a\",\"b\",\"c\",\"a\",\"b\"], the \"nn\" context vector y is [\"b\",\"c\",\"a\",\"b\"]. Chosing \"an\" (for adjacent neighbors) not only includes the next neighbor but also the previous neighbor, every element of y is then a tuple of previous and next neihbor. Returns : y , associated context to data . Additional functions calc_metrics \u2014 Function calc_metrics(model::IB) Computes the different metrics ( H(T), I(X;T), I(Y;T) and L ) of an IB model based on its internal probability distributions. Parameters : model : an IB model Returns : (ht, ixt, iyt, L), metrics. ht is the entropy of the clustered representation. ixt is the mutual information between input data and clustered representation. iyt is the mutual information between context and clustered representation. L is the loss function. search_optima \u2014 Function search_optima!(model::IB, n_iter = 10000) Otimization is not 100% guaranteed to converge to a global maxima . this function initializes and optimizes the provided IB model n_iter times, then, the optimization with the lowest L value is selected. The provided IB is updated in place. Parameters : model : an IB model n_iter ( Int ): defined how many initialization/optimization are performed for the optima search. Returns : nothing . The update is done in place. print_results \u2014 Function print_results(m::IB, disp_thres = 0.1) Displays the results of an optimized IB model. Parameters : m : an IB optimized model disp_thres ( Float ): The probability threshold to consider that a category belongs to a given threshold. This makes reading the results more easy. Defaults to 0.1. Returns : nothing . Print the results. If you want to get the raw probabilities p(t|x) after optimization ( print_results filters it for ease of readability), you can access them with : pt_x = model.qt_x Similarly, you can also get p(y|t) or p(t) with model.qy_t and model.qt . get_IB_curve \u2014 Function `get_IB_curve(m::IB, start = 0.1, stop = 400, step = 0.05; glob = false)` Scans the IB plane with various values of beta to get the optimal curve in the IB plane. Parameters : m : an IB optimized model start ( Float ): The start \u03b2 value. stop ( Float ): The ending \u03b2 value step ( Float ): The steps in \u03b2 values that the function takes upon optimizing the provided model. glob ( Bool : if True, each optimization is done with the help of search_optima (more computationally demanding). Default to False. Returns : (ixt, iyt) the values of mutual information between data and clusters and context and clusters for each \u03b2 value used by the function. Examples Here is a concrete example with data from Bach chorales . The input categories are the 7 types of diatonic chords described in classical music theory. In this case, the data (input series and context) have already been compiled into a co-occurence table, so we instantiate the IB model with a probability distribution: using CategoricalTimeSeries data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"bach_histogram\") bach = DataFrame(CSV.File(data_path)) pxy = Matrix(bach)./sum(Matrix(bach)) #normalizing the co-occurence table to have probabilities. model = IB(pxy, 1000) #instantiating the model with co-occurence probabilities. IB_optimize!(model) print_results(model) The output is in accordance with western music theory. It tells us that we can group category 1, 3 and 6 together: this corresponds to the tonic function in classical harmony. Category 2 and 4 have been clustered together, this is what harmony calls subdominant . Finally category 5 and 7 are joined : this is the dominant function. In the next example, we instantiate the model with a time-series ( saxophone solo ) and define our own context. using CategoricalTimeSeries data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"coltrane_afro_blue\") data = DataFrame(CSV.File(data_path))[!,1] #time-series of notes from saxophone solo (John Coltrane). context = get_y(data, \"an\") # \"an\" stands for adjacent neighbors. model = IB(data, context, 500) # giving the context as input during instantiation. IB_optimize!(model) Now, we show how to plot the IB curve: using Plots using CategoricalTimeSeries data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"bach_histogram\") bach = DataFrame(CSV.File(data_path)) pxy = Matrix(bach)./sum(Matrix(bach)) #normalizing the co-occurence table to have probabilities. model = IB(pxy, 1000) #instantiating the model with co-occurence probabilities. x, y = get_IB_curve(model) a = plot(x, y, color = \"black\", linewidth = 2, label = \"Optimal IB curve\", title = \"Optimal IB curve \\n Bach's chorale dataset\") scatter!(a, x, y, color = \"black\", markersize = 1.7, xlabel = \"I(X;T) \\n\", ylabel = \"- \\n I(Y;T)\", label = \"\", legend = :topleft) Acknowledgments Special thanks to Nori jacoby from whom I learned a lot on the subject. The IB part of this code was tested with his data and reproduces his results. The present implementation is adapted from DJ Strouse's paper https://arxiv.org/abs/1604.00268 and his python implementation.","title":"Data clustering"},{"location":"Data_clustering/#information-bottleneck","text":"The information bottleneck (IB) concept can be used in the context of categorical data analysis to do clustering , or in other words, to look for categories which have equivalent functions. Given a time-series, the IB looks for a concise representation of the data that preserves as much meaningful information as possible. In a sense, it is a lossy compression algorithm. The information to preserve can be seen as the ability to make predictions: given a specific context, how much of what is coming next can we predict ? The goal of this algorithm is to cluster categorical data while preserving predictive power. To learn more about the information bottleneck you can look at [ 1 ] or [ 2 ]","title":"Information bottleneck"},{"location":"Data_clustering/#quick-start","text":"To do a simple IB clustering of categorical, the first step is to instantiate an IB model. Then optimize it via the IB_optimize! function to obtain to obtain the optimal parameters. data = readdlm(\"/path/to/data/\") model = IB(data) #you can call IB(x, beta). beta is a real number that controls the amount of compression. IB_optimize!(model) The data needs to be presented as a 1-D array, otherwise IB interprets it as a probability distribution (see below). To see the results, you can use: print_results(model) Rows are clusters and columns correspond to the input categories. The result is the probability p(t|x) of a category belonging to a given cluster. Since most of the probabilities are very low, print_results sets every p(t|x) > 0.1 to 1 . p(t|x) < 0.1 are set to 0 otherwise for ease of readability (see further usage for more options).","title":"Quick start"},{"location":"Data_clustering/#further-usage","text":"To have a better grasp of the results produced by IB clustering, it is important to understand the parameters influencing the algorithm of IB model structures. The two most important parameters are the amount compression and the definition of the context . They are provided upon instanciation: IB \u2014 Type IB(x, y, \u03b2 = 100, algorithm = \"IB\") IB(x, \u03b2 = 100, algorithm = \"IB\") IB(pxy::Array{Float64,2}, \u03b2 = 100, algorithm = \"IB\") Parameters : x ( Array{Int,1} or Array{Float,1} ): 1-D Array containing input categorical time-series. y ( Array{Any,1} ): Context used for data compression. If not provided, defaults to \"next element\", meaning for each element of x, y represent the next element in the series. This means that the IB model will try to preserve as much information between 'x' and it's next element. (see get_y function) \u03b2 ( Int ): parameter controlling the degree of compression. The smaller \u03b2 is, the more compression. The higher \u03b2 , the bigger the mutual information I(X;T) between the final clusters and original categories is. There are two undesirable situations: if \u03b2 is too small, maximal compression is achieved and all information is lost. If \u03b2 is too high, there is no compression. with \"IB\" algorithm, a high \u03b2 value (~200) is a good starting point. With \"DIB\" algorithm, \u03b2 > ~5 can already be too high to achieve any compression. \u03b2 values > ~1000 break optimization because all metrics are effectively 0. algorithm ( String ): The kind of compression algorithm to use. \"IB\" choses the original IB algorithm (Tishby, 1999) which does soft clustering, \"DIB\" choses the deterministic IB algorithm (DJ Strouse, 2016) doing hard clustering. The former seems to produce more meaningfull clustering. Defaults to \"IB\". pxy ( Array{Float,2} ): joint probability of element 'x' to occur with context 'y'. If not provided, is computed automatically. From x and y . Returns : instance of the IB mutable struct. get_y \u2014 Function get_y(data, type = \"nn\") Defines and return the context associated with the input time-series data . Parameters : data ( Array{Any,1} ): 1-D Array containing input categorical time-series. type ( String ): type of context to use. Possible values are \"nn\" or \"an\". Defaults to \"nn\" (for next neighbor ). This means, if data = [\"a\",\"b\",\"c\",\"a\",\"b\"], the \"nn\" context vector y is [\"b\",\"c\",\"a\",\"b\"]. Chosing \"an\" (for adjacent neighbors) not only includes the next neighbor but also the previous neighbor, every element of y is then a tuple of previous and next neihbor. Returns : y , associated context to data .","title":"Further usage"},{"location":"Data_clustering/#additional-functions","text":"calc_metrics \u2014 Function calc_metrics(model::IB) Computes the different metrics ( H(T), I(X;T), I(Y;T) and L ) of an IB model based on its internal probability distributions. Parameters : model : an IB model Returns : (ht, ixt, iyt, L), metrics. ht is the entropy of the clustered representation. ixt is the mutual information between input data and clustered representation. iyt is the mutual information between context and clustered representation. L is the loss function. search_optima \u2014 Function search_optima!(model::IB, n_iter = 10000) Otimization is not 100% guaranteed to converge to a global maxima . this function initializes and optimizes the provided IB model n_iter times, then, the optimization with the lowest L value is selected. The provided IB is updated in place. Parameters : model : an IB model n_iter ( Int ): defined how many initialization/optimization are performed for the optima search. Returns : nothing . The update is done in place. print_results \u2014 Function print_results(m::IB, disp_thres = 0.1) Displays the results of an optimized IB model. Parameters : m : an IB optimized model disp_thres ( Float ): The probability threshold to consider that a category belongs to a given threshold. This makes reading the results more easy. Defaults to 0.1. Returns : nothing . Print the results. If you want to get the raw probabilities p(t|x) after optimization ( print_results filters it for ease of readability), you can access them with : pt_x = model.qt_x Similarly, you can also get p(y|t) or p(t) with model.qy_t and model.qt . get_IB_curve \u2014 Function `get_IB_curve(m::IB, start = 0.1, stop = 400, step = 0.05; glob = false)` Scans the IB plane with various values of beta to get the optimal curve in the IB plane. Parameters : m : an IB optimized model start ( Float ): The start \u03b2 value. stop ( Float ): The ending \u03b2 value step ( Float ): The steps in \u03b2 values that the function takes upon optimizing the provided model. glob ( Bool : if True, each optimization is done with the help of search_optima (more computationally demanding). Default to False. Returns : (ixt, iyt) the values of mutual information between data and clusters and context and clusters for each \u03b2 value used by the function.","title":"Additional functions"},{"location":"Data_clustering/#examples","text":"Here is a concrete example with data from Bach chorales . The input categories are the 7 types of diatonic chords described in classical music theory. In this case, the data (input series and context) have already been compiled into a co-occurence table, so we instantiate the IB model with a probability distribution: using CategoricalTimeSeries data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"bach_histogram\") bach = DataFrame(CSV.File(data_path)) pxy = Matrix(bach)./sum(Matrix(bach)) #normalizing the co-occurence table to have probabilities. model = IB(pxy, 1000) #instantiating the model with co-occurence probabilities. IB_optimize!(model) print_results(model) The output is in accordance with western music theory. It tells us that we can group category 1, 3 and 6 together: this corresponds to the tonic function in classical harmony. Category 2 and 4 have been clustered together, this is what harmony calls subdominant . Finally category 5 and 7 are joined : this is the dominant function. In the next example, we instantiate the model with a time-series ( saxophone solo ) and define our own context. using CategoricalTimeSeries data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"coltrane_afro_blue\") data = DataFrame(CSV.File(data_path))[!,1] #time-series of notes from saxophone solo (John Coltrane). context = get_y(data, \"an\") # \"an\" stands for adjacent neighbors. model = IB(data, context, 500) # giving the context as input during instantiation. IB_optimize!(model) Now, we show how to plot the IB curve: using Plots using CategoricalTimeSeries data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"bach_histogram\") bach = DataFrame(CSV.File(data_path)) pxy = Matrix(bach)./sum(Matrix(bach)) #normalizing the co-occurence table to have probabilities. model = IB(pxy, 1000) #instantiating the model with co-occurence probabilities. x, y = get_IB_curve(model) a = plot(x, y, color = \"black\", linewidth = 2, label = \"Optimal IB curve\", title = \"Optimal IB curve \\n Bach's chorale dataset\") scatter!(a, x, y, color = \"black\", markersize = 1.7, xlabel = \"I(X;T) \\n\", ylabel = \"- \\n I(Y;T)\", label = \"\", legend = :topleft)","title":"Examples"},{"location":"Data_clustering/#acknowledgments","text":"Special thanks to Nori jacoby from whom I learned a lot on the subject. The IB part of this code was tested with his data and reproduces his results. The present implementation is adapted from DJ Strouse's paper https://arxiv.org/abs/1604.00268 and his python implementation.","title":"Acknowledgments"},{"location":"Motif_recognition/","text":"Motif recognition Time-series sometimes present repeating motifs (or patterns) that are worthwhile identifying. The detection of such motifs can be difficult depending on the amount of noise in the time-series. In the case of categorical time-series, the lack of proper metric to measure distance between motifs can make their detection tricky. Improper distances like the number of differences between the two motifs is commonly used. This package proposes a detection algorithm based on JEREMY BUHLER and MARTIN TOMPA's paper \" Finding Motifs Using Random Projections \". This algorithm although very precise is not exact. Therefore, when you are done detecting potential motifs with the detect_motifs function, you can refine your results with find_motifs for an exact search. The main functions return instances of a class called pattern : pattern \u2014 Class A class storing useful information about found motifs in a time-series. An array of pattern instances is returned when the searching algorithm is done running. Attributes : shape ( Array{Any,1} ): Array containing the shape (or contour) of the first found repetition of the motif. repetitions ( Array{Array{Any,1},1} ): all the different shapes from the motif's repetitions, they can vary a bit from one to the next. positions ( Array{Int,1} ): the positions at which the different repetitions of the motif were found. Main functions detect_motifs \u2014 Function detect_motifs(ts, w, d, t = w - d; iters = 1000, tolerance = 0.95) Detects all motifs of length 'w' occuring more often than chance, being identical to each other up to 'd' differences inside of imput time-series 'ts'. Returns an array of pattern , inside of which the patterns are classified by how frequently they are observed. The first elements is therefore the most frequently observed motif, and so on. Parameters : ts ( Array{Any,1} ): input time-series in which motifs are searched for. w ( Int ): length of motifs to look for. d ( Int ): allowed errors (differences) between motifs repetitions. t = w - d ( Int ): size of the masks to use for random projection in the detection (defaults to w - d). iters = 1000 ( Int ): the numbers of iterations for the random projection process (defaults to 1000) tolerance = 0.95 ( Float ): threshold of motif identification. If set to 1, only matrix entries that are strictly superior to the (probabilistic) threshold are taken into account. Defaults to 0.7, meaning that matrix entries need to be bigger than 0.7*threshold. Returns : motifs : list of pattern instances sorted by frequency of occurence. motifs[1] is therefore the most frequent motif, motifs[2] the second most observed and so on. find_motifs \u2014 Function find_motifs(ts, shape, d) Given a motif of shape 'shape' (array{any,1}), looks for all the repetitions of it which differ only up to 'd' differences inside of the input time-series 'ts'. Input: Parameters : ts ( Array{Any,1} ) : time-series in which to look for motifs shape ( Array{Any,1} ): shape (aray{any,1}) of the motif to look for. d ( Int ): allowed errors (differences) between motifs Returns : motif : an instance of pattern containing the found repetition of the input 'shape'. Plotting To help visualize results, two simple plotting functions are provided. plot_motif \u2014 Function plot_motif(m::pattern) Plots all repetitions of an input pattern instance on top of each other to see how similar they are to each other. Parameters : m : Instance of the pattern class plot_motif \u2014 Function plot_motif(m::pattern, ts) Plots all repetitions of an input pattern instance on top of the input time-series 'ts' to better visualize their repartition in time. Parameters : m : Instance of the pattern class ts ( Array{Any,1} ): Input time-series Example From Michael Brecker's improvisation over the piece \"confirmation\" , we extract a time-series of pitch intervals (difference from one note to the next). A spectral envelope analysis reveals a peak at period 6~7, so we look for motifs of length 7 and allow for 1 error between them. After detection, we visualize the most frequent motif: using DelimitedFiles using CategoricalTimeSeries data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"confirmation\") data = readdlm(data_path) pitch = mod.(data, 12) #Removing octave position: not needed intervals = pitch[2:end] .- pitch[1:end-1] #getting interval time-series. m = detect_motifs(intervals, 7, 1; iters = 700, tolerance = 0.7) plot_motif(m[1]) #plotting most frequent motif We notice that the motif [-1, -2, 10, -10, 2, 3, 5] seems to be the underlying (consensus) shape. In musical notation, this motif would look like this (written in C major): We do an exact search with 1 error allowed to check if our previous detection missed any repetitions, and plot the found motif on top of each other: consensus_shape = [-1, -2, 10, -10, 2, 3, 5] motif = find_motifs(data, consensus_shape, 1) plot_motif(motif) Here, we obtain the same plot as before but this is not necessarily always the case. Knowing the consensus motif usually allows to find its repetitions more efficiently. Now, we visualize the repetitions of the motif in the time-series: plot_motif(motif, ts)","title":"Motif recognition"},{"location":"Motif_recognition/#motif-recognition","text":"Time-series sometimes present repeating motifs (or patterns) that are worthwhile identifying. The detection of such motifs can be difficult depending on the amount of noise in the time-series. In the case of categorical time-series, the lack of proper metric to measure distance between motifs can make their detection tricky. Improper distances like the number of differences between the two motifs is commonly used. This package proposes a detection algorithm based on JEREMY BUHLER and MARTIN TOMPA's paper \" Finding Motifs Using Random Projections \". This algorithm although very precise is not exact. Therefore, when you are done detecting potential motifs with the detect_motifs function, you can refine your results with find_motifs for an exact search. The main functions return instances of a class called pattern : pattern \u2014 Class A class storing useful information about found motifs in a time-series. An array of pattern instances is returned when the searching algorithm is done running. Attributes : shape ( Array{Any,1} ): Array containing the shape (or contour) of the first found repetition of the motif. repetitions ( Array{Array{Any,1},1} ): all the different shapes from the motif's repetitions, they can vary a bit from one to the next. positions ( Array{Int,1} ): the positions at which the different repetitions of the motif were found.","title":"Motif recognition"},{"location":"Motif_recognition/#main-functions","text":"detect_motifs \u2014 Function detect_motifs(ts, w, d, t = w - d; iters = 1000, tolerance = 0.95) Detects all motifs of length 'w' occuring more often than chance, being identical to each other up to 'd' differences inside of imput time-series 'ts'. Returns an array of pattern , inside of which the patterns are classified by how frequently they are observed. The first elements is therefore the most frequently observed motif, and so on. Parameters : ts ( Array{Any,1} ): input time-series in which motifs are searched for. w ( Int ): length of motifs to look for. d ( Int ): allowed errors (differences) between motifs repetitions. t = w - d ( Int ): size of the masks to use for random projection in the detection (defaults to w - d). iters = 1000 ( Int ): the numbers of iterations for the random projection process (defaults to 1000) tolerance = 0.95 ( Float ): threshold of motif identification. If set to 1, only matrix entries that are strictly superior to the (probabilistic) threshold are taken into account. Defaults to 0.7, meaning that matrix entries need to be bigger than 0.7*threshold. Returns : motifs : list of pattern instances sorted by frequency of occurence. motifs[1] is therefore the most frequent motif, motifs[2] the second most observed and so on. find_motifs \u2014 Function find_motifs(ts, shape, d) Given a motif of shape 'shape' (array{any,1}), looks for all the repetitions of it which differ only up to 'd' differences inside of the input time-series 'ts'. Input: Parameters : ts ( Array{Any,1} ) : time-series in which to look for motifs shape ( Array{Any,1} ): shape (aray{any,1}) of the motif to look for. d ( Int ): allowed errors (differences) between motifs Returns : motif : an instance of pattern containing the found repetition of the input 'shape'.","title":"Main functions"},{"location":"Motif_recognition/#plotting","text":"To help visualize results, two simple plotting functions are provided. plot_motif \u2014 Function plot_motif(m::pattern) Plots all repetitions of an input pattern instance on top of each other to see how similar they are to each other. Parameters : m : Instance of the pattern class plot_motif \u2014 Function plot_motif(m::pattern, ts) Plots all repetitions of an input pattern instance on top of the input time-series 'ts' to better visualize their repartition in time. Parameters : m : Instance of the pattern class ts ( Array{Any,1} ): Input time-series","title":"Plotting"},{"location":"Motif_recognition/#example","text":"From Michael Brecker's improvisation over the piece \"confirmation\" , we extract a time-series of pitch intervals (difference from one note to the next). A spectral envelope analysis reveals a peak at period 6~7, so we look for motifs of length 7 and allow for 1 error between them. After detection, we visualize the most frequent motif: using DelimitedFiles using CategoricalTimeSeries data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"confirmation\") data = readdlm(data_path) pitch = mod.(data, 12) #Removing octave position: not needed intervals = pitch[2:end] .- pitch[1:end-1] #getting interval time-series. m = detect_motifs(intervals, 7, 1; iters = 700, tolerance = 0.7) plot_motif(m[1]) #plotting most frequent motif We notice that the motif [-1, -2, 10, -10, 2, 3, 5] seems to be the underlying (consensus) shape. In musical notation, this motif would look like this (written in C major): We do an exact search with 1 error allowed to check if our previous detection missed any repetitions, and plot the found motif on top of each other: consensus_shape = [-1, -2, 10, -10, 2, 3, 5] motif = find_motifs(data, consensus_shape, 1) plot_motif(motif) Here, we obtain the same plot as before but this is not necessarily always the case. Knowing the consensus motif usually allows to find its repetitions more efficiently. Now, we visualize the repetitions of the motif in the time-series: plot_motif(motif, ts)","title":"Example"},{"location":"Spectral_properties/","text":"Spectral Envelope The spectral envelope is a tool to study cyclic behaviors in categorical data. It is more informative than the traditional approach of attributing a different number to each category for power-spectral density estimation. For each frequency in the spectrum, the spectral envelope finds an optimal real-numbered mapping that maximizes the normed power-spectral density at this point. Therefore, no matter what mapping is choosen for the different categories, the power-spectral density will always be bounded by the spectral envelope. The spectral envelope was defined by David S. Stoffer in DAVID S. STOFFER, DAVID E. TYLER, ANDREW J. MCDOUGALL, Spectral analysis for categorical time series: Scaling and the spectral envelope . Main functions spectral_envelope \u2014 Function spectral_envelope(ts; m = 3) Computes the spectral envelope of an input categorical time-series. The degree of smoothing can be chosen by the user. Parameters : ts ( Array{Any,1} ): 1-D Array containing input categorical time-series. m ( Int ): Smoothing parameter. corresponds to how many neighboring points are to be involved in the smoothing (weighted average). Defaults to 3. Returns : (freq, se, eigev) , with freq the frequencies of the power-spectrum, se the values of the spectral envelope for each frequency in 'freq'. eigvecs contains the optimal real-valued mapping for each frequency point. get_mappings \u2014 Function get_mappings(data, freq; m = 3) Computes, for a given frequency freq , the optimal mappings for the categories in data . Scans the vincinity of freq to find the maximum of the spectral envelope, prints a sum up and returns the obtained mappings. Parameters : data ( Array{Any,1} ): 1-D Array containing input categorical time-series. freq ( Float ): Frequency for which the mappings are wanted. The vincinity of 'freq' will be scaned to find maximal value of the spectral envelope. m ( Int ): Smoothing parameter. corresponds to how many neighboring points are to be involved in the smoothing (weighted average). Defaults to 3. Returns : mappings , the optimal mappings for the found maxima around 'freq'. Example Applying the spectral envelope to study a segment of DNA from the Epstein-Barr virus and plotting the results: using DelimitedFiles, Plots using CategoricalTimeSeries data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"DNA_data.txt\") data = readdlm(data_path) f, se, eigvecs = spectral_envelope(data; m = 4) plot(f, se, xlabel = \"Frequency\", ylabel = \"Intensity\", title = \"test data: extract of Epstein virus DNA\", label = \"spectral envelope\") To get the associated optimal mapping for the peak at frequency 0.33: mappings = get_mappings(data, 0.33) >> position of peak: 0.33 strengh of peak: 0.6 print(mappings) >> [\"A : 0.54\", \"G : 0.62\", \"T : -0.57\", \"C : 0.0\"]","title":"Spectral properties"},{"location":"Spectral_properties/#spectral-envelope","text":"The spectral envelope is a tool to study cyclic behaviors in categorical data. It is more informative than the traditional approach of attributing a different number to each category for power-spectral density estimation. For each frequency in the spectrum, the spectral envelope finds an optimal real-numbered mapping that maximizes the normed power-spectral density at this point. Therefore, no matter what mapping is choosen for the different categories, the power-spectral density will always be bounded by the spectral envelope. The spectral envelope was defined by David S. Stoffer in DAVID S. STOFFER, DAVID E. TYLER, ANDREW J. MCDOUGALL, Spectral analysis for categorical time series: Scaling and the spectral envelope .","title":"Spectral Envelope"},{"location":"Spectral_properties/#main-functions","text":"spectral_envelope \u2014 Function spectral_envelope(ts; m = 3) Computes the spectral envelope of an input categorical time-series. The degree of smoothing can be chosen by the user. Parameters : ts ( Array{Any,1} ): 1-D Array containing input categorical time-series. m ( Int ): Smoothing parameter. corresponds to how many neighboring points are to be involved in the smoothing (weighted average). Defaults to 3. Returns : (freq, se, eigev) , with freq the frequencies of the power-spectrum, se the values of the spectral envelope for each frequency in 'freq'. eigvecs contains the optimal real-valued mapping for each frequency point. get_mappings \u2014 Function get_mappings(data, freq; m = 3) Computes, for a given frequency freq , the optimal mappings for the categories in data . Scans the vincinity of freq to find the maximum of the spectral envelope, prints a sum up and returns the obtained mappings. Parameters : data ( Array{Any,1} ): 1-D Array containing input categorical time-series. freq ( Float ): Frequency for which the mappings are wanted. The vincinity of 'freq' will be scaned to find maximal value of the spectral envelope. m ( Int ): Smoothing parameter. corresponds to how many neighboring points are to be involved in the smoothing (weighted average). Defaults to 3. Returns : mappings , the optimal mappings for the found maxima around 'freq'.","title":"Main functions"},{"location":"Spectral_properties/#example","text":"Applying the spectral envelope to study a segment of DNA from the Epstein-Barr virus and plotting the results: using DelimitedFiles, Plots using CategoricalTimeSeries data_path = joinpath(dirname(dirname(pathof(CategoricalTimeSeries))), \"test\", \"DNA_data.txt\") data = readdlm(data_path) f, se, eigvecs = spectral_envelope(data; m = 4) plot(f, se, xlabel = \"Frequency\", ylabel = \"Intensity\", title = \"test data: extract of Epstein virus DNA\", label = \"spectral envelope\") To get the associated optimal mapping for the peak at frequency 0.33: mappings = get_mappings(data, 0.33) >> position of peak: 0.33 strengh of peak: 0.6 print(mappings) >> [\"A : 0.54\", \"G : 0.62\", \"T : -0.57\", \"C : 0.0\"]","title":"Example"},{"location":"misc/","text":"Miscalleneous Here we regroup all additional useful functions that do not necessarily deserve a section of their own. rate_evolution \u2014 Function rate_evolution(series) The rate of evolution is a way to test the stationarity of a categorical time-series. If the rate evolves more or less linearly, then the time-series can reasonably be considered stationary. It is most informative to plot the rate of evolution of each categories on the same graph for a direct visual inspection. Parameters : series ( Array{any,1} ): 1-D Array of categorical time-series. Returns : RATE , Array containing, for each category, an array representing it's rate of evolution. LaggedBivariateProbability \u2014 Function LaggedBivariateProbability(serie, Lags::Array{Int64,1}, Category1, Category2) Returns the lagged bivariate probability of two given categories, Pij. Given i and j two categories, and l a lag (or array of lags), Pij is the probability to have the category j at time t + l, if we have i at time t. Parameters : serie ( Array{any,1} ): 1-D Array of categorical time-series. category1 category2 Returns : pij , Array containing, for each value in lags , the lagged bivariate probability. varcov \u2014 Function varcov(ts::Array{Float64,2}) Computes the covariance-variance matrix of a given multivariate time-series. This can also be used for a univariate time-series but the input should still be 2-D. Parameters : ts ( Array{Float,2} ): 2-D input array of multivariate time-series. Returns : cov_matrix the correpsonding covariance matrix. power_spectrum \u2014 Function power_spectrum(x::Array{Float64,1}, window::Int, step::Int) Computes an estimation of the power-spectrum of the input time-series x . Parameters : x ( Array{Float,1} ): 1-D Array of real-valued time-series. window ( Int ): Integer specifying the size of the window for averaging Must be shorter than length(x). Recommended value is 1/10th of length(x). step ( Int ): Parameters controlling the overlap between the windows. Shouldn't be biggger than div(window,2). Returns : pxx , the estimated power-spectrum.","title":"Miscellaneous"},{"location":"misc/#miscalleneous","text":"Here we regroup all additional useful functions that do not necessarily deserve a section of their own. rate_evolution \u2014 Function rate_evolution(series) The rate of evolution is a way to test the stationarity of a categorical time-series. If the rate evolves more or less linearly, then the time-series can reasonably be considered stationary. It is most informative to plot the rate of evolution of each categories on the same graph for a direct visual inspection. Parameters : series ( Array{any,1} ): 1-D Array of categorical time-series. Returns : RATE , Array containing, for each category, an array representing it's rate of evolution. LaggedBivariateProbability \u2014 Function LaggedBivariateProbability(serie, Lags::Array{Int64,1}, Category1, Category2) Returns the lagged bivariate probability of two given categories, Pij. Given i and j two categories, and l a lag (or array of lags), Pij is the probability to have the category j at time t + l, if we have i at time t. Parameters : serie ( Array{any,1} ): 1-D Array of categorical time-series. category1 category2 Returns : pij , Array containing, for each value in lags , the lagged bivariate probability. varcov \u2014 Function varcov(ts::Array{Float64,2}) Computes the covariance-variance matrix of a given multivariate time-series. This can also be used for a univariate time-series but the input should still be 2-D. Parameters : ts ( Array{Float,2} ): 2-D input array of multivariate time-series. Returns : cov_matrix the correpsonding covariance matrix. power_spectrum \u2014 Function power_spectrum(x::Array{Float64,1}, window::Int, step::Int) Computes an estimation of the power-spectrum of the input time-series x . Parameters : x ( Array{Float,1} ): 1-D Array of real-valued time-series. window ( Int ): Integer specifying the size of the window for averaging Must be shorter than length(x). Recommended value is 1/10th of length(x). step ( Int ): Parameters controlling the overlap between the windows. Shouldn't be biggger than div(window,2). Returns : pxx , the estimated power-spectrum.","title":"Miscalleneous"}]}